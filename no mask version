import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from PIL import Image
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torchvision.models import resnet34, ResNet34_Weights

model_name = "single_angle_model_v10"
num_epochs = 100
batch_size = 16
learning_rate = 0.0001

# Dataset Class
class TireDataset(Dataset):
    def __init__(self, image_paths, depths, transform=None):
        self.image_paths = image_paths
        self.depths = depths
        self.transform = transform
        self.valid_indices = [i for i in range(len(self.image_paths)) if os.path.exists(self.image_paths[i])]

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        image_path = self.image_paths[actual_idx]
        image = Image.open(image_path).convert('RGB')

        depth = self.depths.iloc[actual_idx]

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor([depth], dtype=torch.float)

# Load and split dataset
dataset_folder = '/content/drive/MyDrive/Bachelorprosjekt - Continental/LAST/Dataset/Single angle - training_validation'
excel_file = '/content/drive/MyDrive/Bachelorprosjekt - Continental/Conti - Final folder/Datasett.xlsx'

df = pd.read_excel(os.path.join(dataset_folder, excel_file))
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

train_dataset = TireDataset(
    [os.path.join(dataset_folder, img) for img in train_df["Bilde"]],
    train_df["Dybde (mm)"],
    transform=transforms.Compose([transforms.Resize((1024, 1024)), transforms.ToTensor()])
)

val_dataset = TireDataset(
    [os.path.join(dataset_folder, img) for img in val_df["Bilde"]],
    val_df["Dybde (mm)"],
    transform=transforms.Compose([transforms.Resize((1024, 1024)), transforms.ToTensor()])
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=int(batch_size / 2), shuffle=False)

# Define Model
class SimpleDepthEstimator(nn.Module):
    def __init__(self):
        super(SimpleDepthEstimator, self).__init__()
        self.features = models.resnet34(weights=ResNet34_Weights.DEFAULT)
        self.features.fc = nn.Identity()
        self.depth_layers = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.depth_layers(x)
        return x

model = SimpleDepthEstimator()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)

train_losses = []
val_losses = []
predicted_depths = []
actual_depths = []

# Training and Validation Loop
for epoch in range(num_epochs):
    model.train()
    train_loss_accum = 0
    for batch_idx, (images, depths) in enumerate(train_loader):
        images, depths = images.to(device), depths.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, depths)
        loss.backward()
        optimizer.step()
        train_loss_accum += loss.item()

        if (batch_idx + 1) % 5 == 0:
            print(f'Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')

    average_train_loss = train_loss_accum / len(train_loader)
    train_losses.append(average_train_loss)
    print(f'Epoch {epoch+1}, Average Train Loss: {average_train_loss:.4f}')

    model.eval()
    val_loss_accum = 0
    for batch_idx, (images, depths) in enumerate(val_loader):
        images, depths = images.to(device), depths.to(device)
        images.requires_grad_()
        with torch.no_grad():
            outputs = model(images)
            loss = criterion(outputs, depths)
        val_loss_accum += loss.item()

        if batch_idx == 0:  # Saliency map and depth reporting for the first batch of validation set
            outputs = model(images)
            outputs.backward(torch.ones_like(outputs))
            saliency, _ = torch.max(images.grad.data.abs(), dim=1)
            saliency = saliency[0].cpu().numpy()

            plt.figure(figsize=(15, 4))

            plt.subplot(1, 3, 1)
            plt.imshow(images[0].cpu().detach().permute(1, 2, 0))
            plt.title('Original Image')

            plt.subplot(1, 3, 2)
            plt.imshow(saliency, cmap='hot')
            plt.title('Saliency Map')

            plt.subplot(1, 3, 3)
            plt.imshow(images[0].cpu().detach().permute(1, 2, 0))
            plt.imshow(saliency, cmap='hot', alpha=0.4)
            plt.title('Saliency Map Overlaid')

            plt.show()

        predicted_depths.append(outputs.detach().cpu().numpy())
        actual_depths.append(depths.detach().cpu().numpy())

        if (batch_idx + 1) % 3 == 0:
            print(f'Epoch {epoch+1}, Validation Batch {batch_idx+1}/{len(val_loader)}, Validation Loss: {loss.item():.4f}')

    average_val_loss = val_loss_accum / len(val_loader)
    val_losses.append(average_val_loss)
    print(f'Epoch {epoch+1}, Average Validation Loss: {average_val_loss:.4f}')
    scheduler.step(average_val_loss)

# Save the model
model_save_path = f'/content/drive/MyDrive/Bachelorprosjekt - Continental/LAST/Generated models/{model_name}/{model_name}_final.pth'
torch.save(model.state_dict(), model_save_path)

# Save losses and predictions to CSV
results_df = pd.DataFrame({
    'Train Loss': train_losses,
    'Validation Loss': val_losses,
    'Predicted Depth': [p[0] for p in predicted_depths],
    'Actual Depth': [a[0] for a in actual_depths]
})
results_csv_path = f'/content/drive/MyDrive/Bachelorprosjekt - Continental/LAST/Generated models/{model_name}/{model_name}_results.csv'
results_df.to_csv(results_csv_path, index=False)
